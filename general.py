# -*- coding: utf-8 -*-
"""General.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OSAJIK2-RG09WstH90W_1hgc6Cb-LGrN
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from scipy.stats import multivariate_normal as mvn

def accuracy(y,y_hat):
  return np.mean(y==y_hat)

def coinFlip(y):
  y_hat = np.zeros(len(y))

  for i in range(len(y)):
    flip = np.random.randn(1)
    if flip > 0:
      y_hat[i] = 1
  
  return y_hat

def confusionMatrix(actual, predicted):

    # extract the different classes
    classes = np.unique(actual)

    # initialize the confusion matrix
    confmat = np.zeros((len(classes), len(classes)))

    # loop across the different combinations of actual / predicted classes
    for i in range(len(classes)):
        for j in range(len(classes)):

           # count the number of instances in each combination of actual / predicted classes
           confmat[i, j] = np.sum((actual == classes[i]) & (predicted == classes[j]))

    return confmat

class KNNClassifier():
  def fit(self, x, y):
    self.x = x
    self.y = y

  def predict(self, x, k, epsilon=1e-3):
    N = len(x)  #Number of rows
    y_hat = np.zeros(N)

    for i in range(N):
      dist_sqr = np.sum((self.x - x[i])**2, axis=1) #Get the squared distance of each point
      idxt = np.argsort(dist_sqr)[:k] #Get the indexes of the K nearest neighbors
      gamma_k = 1 / (np.sqrt(dist_sqr[idxt]+epsilon)) #Get the weights

      y_hat[i] = np.bincount(self.y[idxt], weights=gamma_k).argmax()

    return y_hat

class GaussNB():
  def fit(self, x, y, epsilon=1e-3):
    self.likelihoods = dict()
    self.priors = dict()

    #Determine your classes
    self.K = set(y.astype(int))

    #Assign the x values to every given class
    for k in self.K:
      x_k = x[y==k,:]

      #Populate likelihoods
      self.likelihoods[k] = {"Mean" : x_k.mean(axis=0),
                             "Covariance" : x_k.var(axis=0) + epsilon}
      #populate priors (probability of x given y)
      self.priors[k] = len(x_k) / len(x)
    
  def predict(self, x):
    #Get number and dimension of observations
    N, D = x.shape

    #Get the predicted probability for every observation
    p_hat = np.zeros((N,len(self.K)))

    for k,l in self.likelihoods.items():
      p_hat[:,k] = mvn.logpdf(x, l['Mean'], l['Covariance']) + np.log(self.priors[k])

    return p_hat.argmax(axis=1)

class GaussBayes():
  def fit(self, x, y, epsilon=1e-3):
    self.likelihoods = dict()
    self.priors = dict()
    self.K = set(y.astype(int))

    #Set the covariance matrix
    for k in self.K:
      x_k = x[y==k,:]
      N_k, D = x_k.shape
      mu_k = x_k.mean(axis=0)
      
      self.likelihoods[k] = {'Mean' : mu_k,
                             'Covariance' : (1/(N_k-1)) * np.matmul((x_k - mu_k).T, x_k-mu_k) + epsilon*np.identity(D)}
      self.priors[k] = len(x_k) / len(x)

  def predict(self, x):
    #Get number and dimension of observations
    N, D = x.shape

    #Get the predicted probability for every observation
    p_hat = np.zeros((N,len(self.K)))

    for k,l in self.likelihoods.items():
      p_hat[:,k] = mvn.logpdf(x, l['Mean'], l['Covariance']) + np.log(self.priors[k])

    return p_hat.argmax(axis=1)